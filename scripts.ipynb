{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00161049",
   "metadata": {},
   "source": [
    "# Map the sweep IDs to names using the wanbd API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b793d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "\n",
    "# === Settings ===\n",
    "ENTITY = \"matteotolloso\"        # e.g. \"my-team\" or your username\n",
    "PROJECT = \"graph-uncertainty\"      # e.g. \"my-cool-project\"\n",
    "INPUT_CSV = \"/vast/m.tolloso/graph-uncertainty/wandb_export_2025-08-22T12_12_21.994+02_00.csv\"\n",
    "OUTPUT_CSV = \"/vast/m.tolloso/graph-uncertainty/wandb_export_final.csv\"\n",
    "\n",
    "# === Load CSV ===\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# Get unique sweep IDs from the CSV\n",
    "sweep_ids = df[\"Sweep\"].dropna().unique()\n",
    "\n",
    "# === Fetch sweep names from API ===\n",
    "api = wandb.Api()\n",
    "id_to_name = {}\n",
    "\n",
    "for sid in sweep_ids:\n",
    "    try:\n",
    "        sweep = api.sweep(f\"{ENTITY}/{PROJECT}/{sid}\")\n",
    "        id_to_name[sid] = sweep.name or sid  # fallback to ID if no name\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not fetch sweep {sid}: {e}\")\n",
    "        id_to_name[sid] = sid  # fallback to ID\n",
    "\n",
    "# === Replace IDs with names ===\n",
    "df[\"Sweep\"] = df[\"Sweep\"].map(id_to_name)\n",
    "\n",
    "# === Save new CSV ===\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(f\"✅ Updated CSV saved as {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "326e6be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Coauthor CS Dataset with OOD Validation ---\n",
      "Nodes for training (ID only): 8813\n",
      "Nodes for validation (ID+OOD): 3666 -> 2907 ID, 759 OOD\n",
      "Nodes for testing (ID+OOD): 3668 -> 2964 ID, 704 OOD\n",
      "Using DataLoader for full-batch training.\n",
      "--- Chameleon Dataset with OOD Validation ---\n",
      "Nodes for training (ID only): 647\n",
      "Nodes for validation (ID+OOD): 729 -> 438 ID, 291 OOD\n",
      "Nodes for testing (ID+OOD): 456 -> 276 ID, 180 OOD\n",
      "--- Squirrel Dataset with OOD Validation ---\n",
      "Nodes for training (ID only): 1515\n",
      "Nodes for validation (ID+OOD): 1664 -> 982 ID, 682 OOD\n",
      "Nodes for testing (ID+OOD): 1041 -> 622 ID, 419 OOD\n",
      "--- Reddit2 Dataset with OOD Validation ---\n",
      "Nodes for training (ID only): 109517\n",
      "Nodes for validation (ID+OOD): 23699 -> 17343 ID, 6356 OOD\n",
      "Nodes for testing (ID+OOD): 55334 -> 40601 ID, 14733 OOD\n",
      "Using DataLoader for full-batch training.\n",
      "Class Label Intervals:\n",
      "Class 0: [-inf, 2014.0)]\n",
      "Class 1: [2014.0, 2016.0)]\n",
      "Class 2: [2016.0, 2018.0)]\n",
      "Class 3: [2018.0, 2019.0)]\n",
      "Class 4: [2019.0, inf)]\n",
      "--- OGBN-Arxiv (Year) Dataset with OOD Validation ---\n",
      "Nodes for training (ID only): 69523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vast/m.tolloso/miniconda3/envs/gu/lib/python3.11/site-packages/ogb/nodeproppred/dataset.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_dict = torch.load(pre_processed_file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes for validation (ID+OOD): 33868 -> 23415 ID, 10453 OOD\n",
      "Nodes for testing (ID+OOD): 33870 -> 23245 ID, 10625 OOD\n",
      "--- SNAP-Patents (Year) Dataset with OOD Validation ---\n",
      "Nodes for training (ID only): 1053055\n",
      "Nodes for validation (ID+OOD): 584784 -> 351259 ID, 233525 OOD\n",
      "Nodes for testing (ID+OOD): 584785 -> 350730 ID, 234055 OOD\n",
      "Using DataLoader for full-batch training.\n"
     ]
    }
   ],
   "source": [
    "from dataset_loader.dataset_loader import dataset_loader\n",
    "\n",
    "\n",
    "\n",
    "datasets = ['coauthor', 'chameleon', 'squirrel', 'reddit2', 'arxiv', 'patents']\n",
    "\n",
    "for dataset in datasets:\n",
    "    data = dataset_loader(dataset, {})\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
